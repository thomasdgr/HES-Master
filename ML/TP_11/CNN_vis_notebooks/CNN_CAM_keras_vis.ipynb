{"cells":[{"cell_type":"markdown","metadata":{"id":"oKsC6v7NNqWB"},"source":["# Class Activation Map with Keras\n","\n","Examples with Class Activation Map\n","\n","The notebook can be run eiher locally or in Google Colaboratory (<https://colab.research.google.com>)\n","\n","The `ENVIRONMENT` variable must be set accordingly (`LOCAL` or `CLOUD`)\n","\n","If run on the Google Colab, the image must be uploaded first (Left pane -> Files -> UPLOAD).\n","\n","It is better to change the runtime to increase the computation power (Runtime -> Change runtime type -> GPU).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmyy_tRqNqWC"},"outputs":[],"source":["from tensorflow.keras import activations\n","#from keras.applications import vgg16 # NOTE: maybe need to have tensorflow.keras. instead of keras.\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input \n","from tensorflow.keras.applications.imagenet_utils import decode_predictions\n","from tensorflow.keras.preprocessing import image\n","\n","import matplotlib.cm as cm\n","import matplotlib.pyplot as pl\n","import numpy as np\n","%matplotlib inline\n","\n","# Get the path of the notebook\n","from pathlib import Path\n","CURRENT_PATH = Path('.').resolve()\n","\n","ENVIRONMENT = 'CLOUD' # Possible values: 'LOCAL' or 'CLOUD' (for Google Colab)\n","IMAGES_FOLDER = 'images'"]},{"cell_type":"markdown","metadata":{"id":"W9uVDQBeNqWL"},"source":["## Load the model\n","The model is loaded with the ImageNet weigths. The model is taken as is, without re-training any layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-mKy7E-NqWM"},"outputs":[],"source":["model = VGG16(weights='imagenet')"]},{"cell_type":"markdown","metadata":{"id":"TstxBw9ONqWH"},"source":["## Load an image \n","The image is loaded an convert to a numpy array\n","At first the image is loaded in PIL format (width, height, channel) and then transform in numpy (height, width, channel)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7C3fL_sljk0"},"outputs":[],"source":["#IMAGE_2_PROCESS = 'cat01.jpg' # If on the cloud, the image must be uploaded first\n","#IMAGE_2_PROCESS = 'cow.jpg' # If on the cloud, the image must be uploaded first\n","#IMAGE_2_PROCESS = 'miniskirt.jpg' # If on the cloud, the image must be uploaded first\n","#IMAGE_2_PROCESS = 'soccer02.jpg' # If on the cloud, the image must be uploaded first\n","#IMAGE_2_PROCESS = 'soccer03.jpg' # If on the cloud, the image must be uploaded first\n","IMAGE_2_PROCESS = 'cow.jpg' # If on the cloud, the image must be uploaded first\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gS5kY8rVNqWI","scrolled":true},"outputs":[],"source":["filename = f'{CURRENT_PATH}/{IMAGES_FOLDER}/{IMAGE_2_PROCESS}' if ENVIRONMENT == 'LOCAL' else IMAGE_2_PROCESS\n","# Load an image in PIL format\n","original = image.load_img(filename, target_size=(224, 224))\n","# Convert it to a numpy array \n","numpy_image = image.img_to_array(original)\n","# Add a dimension to the image (batchsize, height, width, channels)\n","image_input = np.expand_dims(numpy_image, axis=0)\n","print(f'Image batch size {image_input.shape}')\n","pl.figure(figsize=(8,8))\n","pl.grid(True)\n","pl.imshow(np.uint8(image_input[0]))"]},{"cell_type":"markdown","metadata":{"id":"VFi5J4ohNqWO"},"source":["## Predict classes\n","The image can be processed to match the input of the model and it can do prediction on the processed image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03MN1FySNqWO"},"outputs":[],"source":["# Prepare the image for the model\n","processed_image = preprocess_input(image_input.copy())\n","# Get the predicted probabilities for each class\n","predictions = model.predict(processed_image)\n","# Get the top 5 predictions\n","label = decode_predictions(predictions)\n","print(label)"]},{"cell_type":"markdown","metadata":{"id":"jcclEOcINqWR"},"source":["## CAM visualization\n","\n","The CAM visualization is done using tf-Keras-vis (<https://github.com/keisen/tf-keras-vis/>).\n","\n","The model returns a probability for each classes. We need to know the ID of the class we are predicting in order to apply the right heatmap on the original image.\n","List of mapping between classes and IDs is available here: <https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>.\n","\n","**vanilla**: No modification of backpropagation\n","\n","**guided**: Only propagate positive gradients for positive activations. We are only interested in what the image features the neuron detects, not what it doesn't detect. When propagating the gradient, all negative gradients are set to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qvytv8rWOnw"},"outputs":[],"source":["!pip install tf-keras-vis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAdWtWUdQ6Fz"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","from matplotlib import cm\n","from tf_keras_vis.gradcam import Gradcam\n","from tf_keras_vis.utils.model_modifiers import ReplaceToLinear, GuidedBackpropagation\n","from tf_keras_vis.utils.scores import CategoricalScore\n","\n","gradcams = [\n","    # Not guided\n","    Gradcam(model, model_modifier=[\n","      ReplaceToLinear(),\n","    ], clone=True),\n","    \n","    # Guided\n","    Gradcam(model, model_modifier=[\n","        ReplaceToLinear(),\n","        GuidedBackpropagation()\n","    ], clone=True),\n","]\n","\n","IMAGE_INDEX = np.argmax(predictions)\n","\n","# We create a list of Class Activation Map (CAM) made with guided and \n","# without guided backprop  \n","cam_list = [\n","    gradcam(CategoricalScore(IMAGE_INDEX), numpy_image)\n","    for gradcam in gradcams\n","]\n","\n","fig, ax = pl.subplots(1, len(cam_list), figsize=(18, 8))\n","\n","pl.figure(figsize=(18,8))\n","\n","titles = [\"vanilla\", \"guided\"]\n","\n","# We plot the heatmaps\n","for cam_idx, cam in enumerate(cam_list):\n","    ax[cam_idx].set_title(titles[cam_idx])\n","    ax[cam_idx].imshow(np.uint8(image_input[0]))\n","    heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n","    ax[cam_idx].imshow(heatmap, cmap='jet', alpha=0.5) # overlay"]},{"cell_type":"markdown","metadata":{"id":"_HpsawMGLmW5"},"source":["## Questions\n","\n","1. Compute the activation map for all the images we give you.\n","2. Do the maps reflect the class of the pictures?\n","3. Are there images in which the activation maps highlight zones that do not belong to the object? Which ones?\n","4. Look at the provided images and observe the images that were used to train the classifier on these specific classes (i.e., soccer ball). Do you think that all images have been labelled appropriately ? explain.\n","5. Hypothesize about the behavior of the network observed in 3."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}