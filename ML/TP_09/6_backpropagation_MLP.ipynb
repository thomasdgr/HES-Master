{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation and the MLP\n",
    "From: https://rolisz.ro/2013/04/18/neural-networks-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The following script allows you to create a 2D dataset by using the mouse. The left click adds points belonging to class A (blue), and the right click adds points belonging to class B (red). You can create as many points as you desire. The final dataset will contain hence three values per point: x coordinate (-1 ≤ x ≤ 1), y coordinate (-1 ≤ y ≤ 1) and the class ∈ {1,-1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = pl.figure(figsize=(6,6))\n",
    "pl.title(\"Input Dataset\")\n",
    "pl.xlim((-1.2,1.2))\n",
    "pl.ylim((-1.2,1.2))\n",
    "\n",
    "dataset = []\n",
    "\n",
    "def onclick(event):\n",
    "    global dataset\n",
    "    cx = event.xdata\n",
    "    cy = event.ydata\n",
    "    co = event.button\n",
    "    dataset.append((cx, cy, co-2))\n",
    "\n",
    "    pl.scatter(cx, cy, c=(['b', 'r'])[co > 2], s=100, lw=0)\n",
    "    pl.grid(True)\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.canvas.mpl_disconnect(onclick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLP\n",
    "The class presented here was adapted from the code found at: https://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "\n",
    "The class implements a MLP with a fully configurable number of layers and neurons. It adapts its weights using the backpropagation algorithm in an online manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    '''\n",
    "    This code was adapted from:\n",
    "    https://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "    '''\n",
    "    def __tanh(self, x):\n",
    "        '''Hyperbolic tangent function'''\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        '''Hyperbolic tangent derivative'''\n",
    "        return 1.0 - a**2\n",
    "\n",
    "    def __logistic(self, x):\n",
    "        '''Sigmoidal function'''\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_derivative(self, a):\n",
    "        '''sigmoidal derivative'''\n",
    "        return a * ( 1 - a )\n",
    "    \n",
    "    def __init__(self, layers, activation='tanh'):\n",
    "        '''\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        '''\n",
    "        self.n_inputs = layers[0]                               # Number of inputs (first layer)\n",
    "        self.n_outputs = layers[-1]                             # Number of ouputs (last layer)\n",
    "        self.layers = layers\n",
    "                                                                # Activation function\n",
    "        if activation == 'logistic':\n",
    "            self.activation = self.__logistic\n",
    "            self.activation_deriv = self.__logistic_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.__tanh\n",
    "            self.activation_deriv = self.__tanh_deriv\n",
    "\n",
    "        self.init_weights()                                     # Initialize the weights of the MLP\n",
    "        \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        This function creates the matrix of weights and initialiazes their values to small values\n",
    "        '''\n",
    "        self.weights = []                                       # Start with an empty list\n",
    "        for i in range(1, len(self.layers) - 1):                # Iterates through the layers\n",
    "                                                                # np.random.random((M, N)) returns a MxN matrix\n",
    "                                                                # of random floats in [0.0, 1.0).\n",
    "                                                                # (self.layers[i] + 1) is number of neurons in layer i plus the bias unit\n",
    "            self.weights.append((2 * np.random.random((self.layers[i - 1] + 1, self.layers[i])) - 1) * 0.25)\n",
    "                                                                # delta_weights are initialized to zero\n",
    "                                                                # Append a last set of weigths connecting the output of the network\n",
    "        self.weights.append((2 * np.random.random((self.layers[i] + 1, self.layers[i + 1])) - 1) * 0.25)\n",
    "   \n",
    "    def fit(self, data_train, data_test=None, learning_rate=0.1, epochs=100):\n",
    "        '''\n",
    "        Online learning.\n",
    "        :param data_train: A tuple (X, y) with input data and targets for training\n",
    "        :param data_test: A tuple (X, y) with input data and targets for testing\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        '''\n",
    "        X = np.atleast_2d(data_train[0])                        # Inputs for training\n",
    "        #temp = np.ones([X.shape[0], X.shape[1]+1])              # Append the bias unit to the input layer\n",
    "        #temp[:, 0:-1] = X                                       \n",
    "        #X = temp                                                # X contains now the inputs plus a last column of ones (bias unit)\n",
    "        y = np.array(data_train[1])                             # Targets for training\n",
    "        error_train = np.zeros(epochs)                          # Initialize the array to store the error during training (epochs)\n",
    "        if data_test is not None:                               # If the test data is provided\n",
    "            error_test = np.zeros(epochs)                       # Initialize the array to store the error during testing (epochs)\n",
    "            out_test = np.zeros(data_test[1].shape)             # Initialize the array to store the output during testing\n",
    "            \n",
    "        a = []                                                  # Create a list of arrays of activations\n",
    "        for l in self.layers:\n",
    "            a.append(np.zeros(l))                               # One array of zeros per layer\n",
    "            \n",
    "        for k in range(epochs):                                 # Iterate through the epochs\n",
    "            error_it = np.zeros(X.shape[0])                     # Initialize an array to store the errors during training (n examples)\n",
    "            for it in range(X.shape[0]):                        # Iterate through the examples in the training set\n",
    "                i = np.random.randint(X.shape[0])               # Select one random example\n",
    "                a[0] = X[i]                                     # The activation of the first layer is the input values of the example\n",
    "\n",
    "                                                                # Feed-forward\n",
    "                for l in range(len(self.weights)):              # Iterate and compute the activation of each layer\n",
    "                    a[l] = np.concatenate((a[l], np.ones(1)))   # Append one for the bias\n",
    "                    a[l+1] = self.activation(np.dot(a[l], self.weights[l])) # Apply the activation function to the product input.weights\n",
    "                \n",
    "                error = a[-1] - y[i]                            # Compute the error: output - target\n",
    "                error_it[it] = np.mean(error ** 2)              # Store the error of this iteration (average of all the outputs)\n",
    "                deltas = [error * self.activation_deriv(a[-1])] # Ponderate the error by the derivative = delta\n",
    "                \n",
    "                                                                # Back-propagation\n",
    "                                                                # We need to begin at the layer previous to the last one (out->in)\n",
    "                for l in range(len(a) - 2, 0, -1):              # Append a delta for each layer\n",
    "                    deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(a[l]))\n",
    "                    deltas[-1] = deltas[-1][:-1]                # delete the delta of the bias since bias units are not connected backwards\n",
    "                deltas.reverse()                                # Reverse the list (in->out)\n",
    "\n",
    "                                                                # Update\n",
    "                for i in range(len(self.weights)):              # Iterate through the layers\n",
    "                    layer = np.atleast_2d(a[i])                 # Activation\n",
    "                    delta = np.atleast_2d(deltas[i])            # Delta\n",
    "                                                                # Compute the weight change using the delta for this layer\n",
    "                                                                # and the change computed for the previous example for this layer\n",
    "                    delta_weights = -learning_rate * layer.T.dot(delta)\n",
    "                    self.weights[i] += delta_weights            # Update the weights\n",
    "                \n",
    "            error_train[k] = np.mean(error_it)                  # Compute the average of the error of all the examples\n",
    "            if data_test is not None:                           # If a testing dataset was provided\n",
    "                error_test[k], _ = self.compute_MSE(data_test)  # Compute the testing error after iteration k\n",
    "            \n",
    "        if data_test is None:                                   # If only a training data was provided\n",
    "            return error_train                                  # Return the error during training\n",
    "        else:\n",
    "            return (error_train, error_test)                    # Otherwise, return both training and testing error\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Evaluates the network for a single observation\n",
    "        '''\n",
    "        a = np.array(x)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            temp = np.ones(a.shape[0]+1)          # append the activation of the bias unit (1)\n",
    "            temp[0:-1] = a\n",
    "            a = self.activation(np.dot(temp, self.weights[l]))\n",
    "        return a\n",
    "    \n",
    "    def compute_output(self, data):\n",
    "        '''\n",
    "        Evaluates the network for a dataset with multiple observations\n",
    "        '''\n",
    "        assert len(data.shape) == 2, 'data must be a 2-dimensional array'\n",
    "\n",
    "        out = np.zeros((data.shape[0], self.n_outputs))\n",
    "        for r in np.arange(data.shape[0]):\n",
    "            out[r,:] = self.predict(data[r,:])\n",
    "        return out\n",
    "    \n",
    "    def compute_MSE(self, data_test):\n",
    "        '''\n",
    "        Evaluates the network for a given dataset and\n",
    "        computes the error between the target data provided\n",
    "        and the output of the network\n",
    "        '''\n",
    "        assert len(data_test[0].shape) == 2, 'data[0] must be a 2-dimensional array'\n",
    "\n",
    "        out = self.compute_output(data_test[0])\n",
    "        return (np.mean((data_test[1] - out) ** 2), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "Let use the MLP class to solve a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = dataset[:,0:2]\n",
    "output_data = dataset[:,2]\n",
    "nn = MLP([2,2,1], 'tanh')\n",
    "# 2 inputs\n",
    "# 2 hidden nodes\n",
    "# 1 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use backpropagation to find the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = nn.fit((input_data, output_data), learning_rate=0.001, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the error through the iterations of the algorithm\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.ylabel('Error (MSE)')\n",
    "pl.xlabel('Epochs')\n",
    "pl.title('Mean Squared Error through epochs')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Now, let's use the trained network to predict the classes in the dataset.\n",
    "This section shows three different manners of showing the outputs of the network, and how to compare its output with the targets to evaluate the performance of the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the output of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn.compute_output(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot boxplots of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8,6))\n",
    "pl.boxplot(output[output_data==-1.0], positions=[1], labels=['A'])\n",
    "pl.boxplot(output[output_data==1.0], positions=[2], labels=['B'])\n",
    "pl.xlabel('Targets')\n",
    "pl.ylabel('MLP output')\n",
    "pl.title(\"Distribution of the network's output wrt the target\")\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the output of the network in the space of features (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = np.arange(-1.2, 1.2, 0.1)\n",
    "input_y = np.arange(-1.2, 1.2, 0.1)\n",
    "input_x_matrix, input_y_matrix = np.meshgrid(input_x, input_y)\n",
    "inputs_xy = np.concatenate((input_x_matrix.flatten()[:,np.newaxis], input_y_matrix.flatten()[:,np.newaxis]), axis=1)\n",
    "\n",
    "output_values = nn.compute_output(inputs_xy)\n",
    "output_matrix = np.reshape(output_values, input_x_matrix.shape)\n",
    "\n",
    "pl.figure(figsize=(8,8))\n",
    "pl.imshow(np.flipud(output_matrix), interpolation='None', extent=(-1.2,1.2,-1.2,1.2))\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[int(d>0)] for d in output_data], s=100)\n",
    "pl.title('Continuous network output and targets');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot targets and predicted outputs separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12,5))\n",
    "pl.subplot(1,2,1)\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[int(d>0)] for d in output_data], s=100)\n",
    "pl.xlim(-1.2, 1.2)\n",
    "pl.ylim(-1.2, 1.2)\n",
    "pl.grid()\n",
    "pl.title('Targets')\n",
    "pl.subplot(1,2,2)\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[int(d>0)] for d in output[:,0]], s=100)\n",
    "pl.xlim(-1.2, 1.2)\n",
    "pl.ylim(-1.2, 1.2)\n",
    "pl.grid()\n",
    "pl.title('MLP output');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Modify the class MLP provided in this notebook to include the momentum term in the function `fit`:\n",
    "\n",
    "~~~\n",
    "fit(self, data_train, data_test=None, learning_rate=0.1, momentum=0.5, epochs=100):\n",
    "~~~\n",
    "\n",
    "Modify the behaviour of the function accordingly. Use the `momentum` constant to modulate the previous Δweights which has to be added to the current Δweights.\n",
    "\n",
    "Save the resulting code in a file named `mlp_backprop_momentum.py`. This file will be used in the next laboratory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
