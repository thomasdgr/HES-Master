{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross-validation\n",
    "In k-fold cross-validation the dataset is split in K parts: k-1 parts are used during training and the remaining part is used for testing the generalization capabilities of the model. This method has the advantage of giving more consistent results than hold-out validation. In this notebook you are going to explore the behaviour of k-fold cross-validation by simulating datasets with diverse degrees of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import sys\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "This function creates a dataset with two classes in two dimensions. It has two parameters: the size of the dataset and the spread of each one of the classes. A high spread value makes both classes to superpose, making the classification more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n, s):\n",
    "    n1 = int(np.ceil(n / 2.0))\n",
    "    n2 = int(np.floor(n / 2.0))\n",
    "    x1 = np.random.normal(-1, s, n1)\n",
    "    y1 = np.random.uniform(-1, 1,  n1)\n",
    "    x2 = np.random.normal(1, s, n2)\n",
    "    y2 = np.random.uniform(-1, 1, n2)\n",
    "    return np.stack((np.concatenate((x1, x2)), np.concatenate((y1, y2)), np.concatenate((np.ones(n1), -1*np.ones(n2)))), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(s):\n",
    "    dataset = create_dataset(200, s)\n",
    "    pl.scatter(dataset[:,0], dataset[:,1], c=[(['b', 'r'])[int(cl > 0)] for cl in dataset[:,2]])\n",
    "    pl.xlim(-3,3)\n",
    "    pl.ylim(-1,1)\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_dataset, s=widgets.FloatSlider(value=0.1, min=0.1, max=1.0, step=0.01, description='Spread:',));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlp_backprop_momentum as mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring k-fold cross-validation\n",
    "The following function splits the dataset in K parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, n_parts=5):\n",
    "    n_rows = dataset.shape[0]\n",
    "    index_all = np.arange(n_rows)\n",
    "    np.random.shuffle(index_all)\n",
    "    parts = []\n",
    "    current_start = 0\n",
    "    for p in np.arange(n_parts):\n",
    "        current_end = current_start + int(np.floor(n_rows / (n_parts-p)))\n",
    "        parts.append(dataset[index_all[current_start:current_end],:])\n",
    "        n_rows -= current_end - current_start\n",
    "        current_start = current_end\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(mlp, dataset, K=5, learning_rate=0.01, momentum=0.7, epochs=100):\n",
    "    MSE_train_mean = 0.0\n",
    "    MSE_test_mean = 0.0\n",
    "\n",
    "    parts = split_dataset(dataset, K)\n",
    "    \n",
    "    for k in np.arange(K):\n",
    "        mlp.init_weights()\n",
    "        \n",
    "        training_parts = set(np.arange(K))\n",
    "        training_parts.remove(k)\n",
    "        dataset_train = np.concatenate([parts[i] for i in list(training_parts)])\n",
    "        dataset_test = parts[k]\n",
    "\n",
    "        input_data = dataset_train[:,0:nn.n_inputs]\n",
    "        output_data = dataset_train[:,nn.n_inputs:(nn.n_inputs+nn.n_outputs)]\n",
    "        input_data_test = dataset_test[:,0:nn.n_inputs]\n",
    "        output_data_test = dataset_test[:,nn.n_inputs:(nn.n_inputs+nn.n_outputs)]\n",
    "        \n",
    "        MSE_train = mlp.fit((input_data, output_data),\n",
    "                            learning_rate=learning_rate, momentum=momentum, epochs=epochs)\n",
    "        temp, _ = mlp.compute_MSE((input_data, output_data))\n",
    "        MSE_train_mean += temp\n",
    "        temp, _ = mlp.compute_MSE((input_data_test, output_data_test))\n",
    "        MSE_test_mean += temp\n",
    "\n",
    "    return (MSE_train_mean / K, MSE_test_mean / K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "In this experiment we create datasets with different degrees of complexity and we test the behaviour of k-fold cross-validation with each one of them. For each dataset, we split the dataset several times, which generates different partitions training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "DATASET_SIZE = 200\n",
    "EPOCHS = 20\n",
    "N_NEURONS = 2\n",
    "K = 5\n",
    "LEARNING_RATE = 0.001\n",
    "MOMENTUM = 0.7\n",
    "DATA_PARAMS = np.arange(0.4, 0.71, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_train = np.zeros((len(DATA_PARAMS), N_SPLITS))\n",
    "MSE_test = np.zeros((len(DATA_PARAMS), N_SPLITS))\n",
    "nn = mlp.MLP([2,N_NEURONS,1], 'tanh')\n",
    "\n",
    "for p, s in enumerate(DATA_PARAMS):                                     # looping the set of parameters\n",
    "    print('Testing dataset with variance:', s)\n",
    "\n",
    "    dataset = create_dataset(DATASET_SIZE, s)\n",
    "\n",
    "    for d in np.arange(N_SPLITS):                                       # looping the splits\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "        temp1, temp2 = k_fold_cross_validation(nn,\n",
    "                                               dataset,\n",
    "                                               K=K,\n",
    "                                               learning_rate=LEARNING_RATE,\n",
    "                                               momentum=MOMENTUM,\n",
    "                                               epochs=EPOCHS)\n",
    "        MSE_train[p,d] = temp1\n",
    "        MSE_test[p,d] = temp2\n",
    "    print(N_SPLITS, ' tests done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the resulting MSE, we can observe that each partition, i.e., each run of cross-validation, generates values of model error which are closer than the results found when using hold-out validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.boxplot(MSE_test.T, positions=DATA_PARAMS, widths=0.05)\n",
    "for c in np.arange(MSE_test.shape[1]):\n",
    "    pl.scatter(DATA_PARAMS, MSE_test[:,c], c='g', marker='x')\n",
    "pl.xlim(np.min(DATA_PARAMS)-0.1, np.max(DATA_PARAMS)+0.1)\n",
    "pl.xlabel('Spread')\n",
    "pl.ylabel('MSE')\n",
    "pl.title('Several runs of cross-validation')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "state": {
    "a739b242888344b6923250c5935ce381": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
