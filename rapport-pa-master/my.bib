@misc{RAG_benchmarking,
	title = {Benchmarking {Large} {Language} {Models} in {Retrieval}-{Augmented} {Generation}},
	url = {http://arxiv.org/abs/2309.01431},
	doi = {10.48550/arXiv.2309.01431},
	abstract = {Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
	month = dec,
	year = {2023},
	note = {arXiv:2309.01431 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chatbot_reinforcement_learning,
	title = {Reinforcement {Learning} for {Optimizing} {RAG} for {Domain} {Chatbots}},
	url = {http://arxiv.org/abs/2401.06800},
	doi = {10.48550/arXiv.2401.06800},
	abstract = {With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Kulkarni, Mandar and Tangarajan, Praveen and Kim, Kyung and Trivedi, Anusua},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06800 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@inproceedings{RAG_recent_advances,
	address = {New York, NY, USA},
	series = {{SIGIR} '22},
	title = {Recent {Advances} in {Retrieval}-{Augmented} {Text} {Generation}},
	isbn = {9781450387323},
	url = {https://doi.org/10.1145/3477495.3532682},
	doi = {10.1145/3477495.3532682},
	abstract = {Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.},
	urldate = {2024-03-18},
	booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Deng and Wang, Yan and Liu, Lemao and Shi, Shuming},
	month = jul,
	year = {2022},
	keywords = {information retrieval, text generation},
	pages = {3417--3419},
}

@misc{RAG_survey,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
	month = jan,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{swiss_ai_center,
	title = {Swiss {AI} {Center}},
	url = {https://frontend-core-engine-swiss-ai-center.kube.isc.heia-fr.ch/},
	abstract = {Frontend app for the Swiss AI Center Project},
	urldate = {2024-03-18},
}

@misc{swiss_ai_center_chatbot,
	title = {Streamlit},
	url = {https://chatbot-ollama-swiss-ai-center.kube.isc.heia-fr.ch/},
	urldate = {2024-03-18},
}

@misc{Andrej_Karpathy,
	title = {Let's build the {GPT} {Tokenizer}},
	author = {Karpathy, Andrej},
	url = {https://www.youtube.com/watch?v=zduSFxRajkE},
	abstract = {The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizer...},
	language = {en},
	urldate = {2024-03-18},
}

@misc{tokenizer_choice,
	title = {Tokenizer {Choice} {For} {LLM} {Training}: {Negligible} or {Crucial}?},
	shorttitle = {Tokenizer {Choice} {For} {LLM} {Training}},
	url = {http://arxiv.org/abs/2310.08754},
	doi = {10.48550/arXiv.2310.08754},
	abstract = {The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model's downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-only tokenizers have been applied to the training of multi-lingual LLMs, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68\%, due to an inefficient tokenization vocabulary.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Ali, Mehdi and Fromm, Michael and Thellmann, Klaudia and Rutmann, Richard and Lübbering, Max and Leveling, Johannes and Klug, Katrin and Ebert, Jan and Doll, Niclas and Buschhoff, Jasper Schulze and Jain, Charvi and Weber, Alexander Arno and Jurkschat, Lena and Abdelwahab, Hammam and John, Chelsea and Suarez, Pedro Ortiz and Ostendorff, Malte and Weinbach, Samuel and Sifa, Rafet and Kesselheim, Stefan and Flores-Herr, Nicolas},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08754 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{greedy_vs_beam_search,
	title = {A {Comparison} of {Greedy} {Search} {Algorithms}},
	volume = {1},
	copyright = {Copyright (c) 2010 Proceedings of the International Symposium on Combinatorial Search},
	issn = {2832-9163},
	url = {https://ojs.aaai.org/index.php/SOCS/article/view/18182},
	doi = {10.1609/socs.v1i1.18182},
	abstract = {We discuss the relationships between three approaches to greedy heuristic search: best-first, hill-climbing, and beam search.  We consider the design decisions within each family and point out their oft-overlooked similarities. We consider the following best-first searches: weighted A*, greedy search, ASeps, window A* and multi-state commitment k-weighted A*.  For hill climbing algorithms, we consider enforced hill climbing and LSS-LRTA*.  We also consider a variety of beam searches, including BULB and beam-stack search. We show how to best configure beam search in order to maximize robustness.  An empirical analysis on six standard benchmarks reveals that beam search and best-first search have remarkably similar performance, and outperform hill-climbing approaches in terms of both time to solution and solution quality.  Of these, beam search is preferable for very large problems and best first search is better on problems where the goal cannot be reached from all states.},
	language = {en},
	number = {1},
	urldate = {2024-03-18},
	journal = {Proceedings of the International Symposium on Combinatorial Search},
	author = {Wilt, Christopher and Thayer, Jordan and Ruml, Wheeler},
	month = aug,
	year = {2010},
	keywords = {Empirical Analysis},
	pages = {129--136},
}


@misc{neural_text_generation,
	title = {Neural {Text} {Generation} with {Unlikelihood} {Training}},
	url = {http://arxiv.org/abs/1908.04319},
	doi = {10.48550/arXiv.1908.04319},
	abstract = {Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-\$k\$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
	month = sep,
	year = {2019},
	note = {arXiv:1908.04319 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
}


@misc{LLM_overview,
	title = {A {Comprehensive} {Overview} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.06435},
	doi = {10.48550/arXiv.2307.06435},
	abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
	month = feb,
	year = {2024},
	note = {arXiv:2307.06435 [cs]},
	keywords = {Computer Science - Computation and Language},
}


@misc{HG_llm_leaderboard,
	title = {Open {LLM} {Leaderboard} - a {Hugging} {Face} {Space} by {HuggingFaceH4}},
	url = {https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard},
	abstract = {Track, rank and evaluate open LLMs and chatbots},
	urldate = {2024-03-22},
}


@misc{HELM_evaluation,
	title = {Holistic {Evaluation} of {Language} {Models} ({HELM})},
	url = {https://crfm.stanford.edu/helm/lite/latest/#/leaderboard},
	abstract = {The Holistic Evaluation of Language Models (HELM) serves as a living benchmark for transparency in language models. Providing broad coverage and recognizing incompleteness, multi-metric measurements, and standardization. All data and analysis are freely accessible on the website for exploration and study.},
	urldate = {2024-03-22},
}


@misc{RAG_for_NLP_tasks,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@misc{LLM_context,
	title = {{YaRN}: {Efficient} {Context} {Window} {Extension} of {Large} {Language} {Models}},
	shorttitle = {{YaRN}},
	url = {http://arxiv.org/abs/2309.00071},
	doi = {10.48550/arXiv.2309.00071},
	abstract = {Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	month = nov,
	year = {2023},
	note = {arXiv:2309.00071 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}


@misc{HG_LLM_elo,
	title = {{LMSys} {Chatbot} {Arena} {Leaderboard} - a {Hugging} {Face} {Space} by lmsys},
	url = {https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard},
	abstract = {Discover amazing ML apps made by the community},
	urldate = {2024-04-08},
}


@misc{gpt3_embedding,
	title = {Text {Embeddings} by {Weakly}-{Supervised} {Contrastive} {Pre}-training},
	url = {http://arxiv.org/abs/2212.03533},
	doi = {10.48550/arXiv.2212.03533},
	abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
	month = feb,
	year = {2024},
	note = {arXiv:2212.03533 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{bgem3_embedding,
	title = {{BGE} {M3}-{Embedding}: {Multi}-{Lingual}, {Multi}-{Functionality}, {Multi}-{Granularity} {Text} {Embeddings} {Through} {Self}-{Knowledge} {Distillation}},
	shorttitle = {{BGE} {M3}-{Embedding}},
	url = {http://arxiv.org/abs/2402.03216},
	doi = {10.48550/arXiv.2402.03216},
	abstract = {In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
	month = feb,
	year = {2024},
	note = {arXiv:2402.03216 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{e5_embedding,
	title = {Text and {Code} {Embeddings} by {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2201.10005},
	doi = {10.48550/arXiv.2201.10005},
	abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
	month = jan,
	year = {2022},
	note = {arXiv:2201.10005 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}


@misc{gao_retrieval_augmented_2024,
	title = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Retrieval-{Augmented} {Generation} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}